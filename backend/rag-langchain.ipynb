{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community --quiet\n",
    "!pip install langchain-huggingface --quiet\n",
    "!pip install einops --quiet\n",
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain_community.document_loaders import *\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA设备\n",
    "CUDA_device = 'cuda:0,1'\n",
    "\n",
    "# 文件路径和模型路径\n",
    "Embedding_Model = 'intfloat/multilingual-e5-large-instruct'\n",
    "LLM_Model = '/kaggle/input/llama-3.1/transformers/8b-instruct/2'\n",
    "file_paths = ['/kaggle/input/20230916test/me.txt', \"/kaggle/input/20230916test/2024-Wealth-Outlook-MidYear-Edition.pdf\", \"/kaggle/input/20230916test/elon-musk-tesla-spacex-and-the-quest-for-a-fantastic-future-0062469673-9780062469670_compress.pdf\"]\n",
    "store_path = '/kaggle/working/me.faiss'\n",
    "\n",
    "# 定义文件扩展名和加载器类的映射关系\n",
    "LOADER_MAPPING = {\n",
    "    '.pdf': PyPDFLoader,\n",
    "    '.txt': TextLoader,\n",
    "    '.md': UnstructuredMarkdownLoader,\n",
    "    '.csv': CSVLoader,\n",
    "    '.jpg': UnstructuredImageLoader,\n",
    "    '.jpeg': UnstructuredImageLoader,\n",
    "    '.png': UnstructuredImageLoader,\n",
    "    '.json': JSONLoader,\n",
    "    '.html': BSHTMLLoader,\n",
    "    '.htm': BSHTMLLoader\n",
    "}\n",
    "\n",
    "def load_single_file(file_path):\n",
    "    \"\"\"\n",
    "    加载单个文件的内容。\n",
    "    参数:\n",
    "        file_path (str): 文件路径\n",
    "    返回:\n",
    "        docs (list): 文档内容列表\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "    loader_class = LOADER_MAPPING.get(ext)\n",
    "    if not loader_class:\n",
    "        print(f\"不支持的文件类型: {ext}\")\n",
    "        return None\n",
    "    loader = loader_class(file_path)\n",
    "    docs = list(loader.lazy_load())\n",
    "    return docs\n",
    "\n",
    "def load_files(file_paths: list):\n",
    "    \"\"\"\n",
    "    批量加载多个文件的内容。\n",
    "    参数:\n",
    "        file_paths (list): 文件路径列表\n",
    "    返回:\n",
    "        docs (list): 所有加载的文档内容\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        return []\n",
    "    \n",
    "    docs = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        print(\"加载文档:\", file_path)\n",
    "        loaded_docs = load_single_file(file_path)\n",
    "        if loaded_docs:\n",
    "            docs.extend(loaded_docs)\n",
    "    return docs\n",
    "\n",
    "def split_text(txt, chunk_size=200, overlap=20):\n",
    "    \"\"\"\n",
    "    将文本分割为多个小段，以便进行进一步处理。\n",
    "    参数:\n",
    "        txt (str): 文本内容\n",
    "        chunk_size (int): 每个段落的字符数\n",
    "        overlap (int): 重叠字符数\n",
    "    返回:\n",
    "        docs (list): 分割后的文档段落列表\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return None\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    docs = splitter.split_documents(txt)\n",
    "    return docs\n",
    "\n",
    "def create_embedding_model(model_file):\n",
    "    \"\"\"\n",
    "    创建嵌入模型以生成文本的嵌入表示。\n",
    "    参数:\n",
    "        model_file (str): 嵌入模型路径\n",
    "    返回:\n",
    "        embedding (HuggingFaceEmbeddings): 嵌入模型实例\n",
    "    \"\"\"\n",
    "    embedding = HuggingFaceEmbeddings(model_name=model_file, model_kwargs={'trust_remote_code': True})\n",
    "    return embedding\n",
    "\n",
    "def save_file_paths(store_path, file_paths):\n",
    "    \"\"\"\n",
    "    将文件路径保存到本地。\n",
    "    参数:\n",
    "        store_path (str): 存储路径\n",
    "        file_paths (list): 文件路径列表\n",
    "    \"\"\"\n",
    "    joblib.dump(file_paths, f'{store_path}/file_paths.pkl')\n",
    "\n",
    "def load_file_paths(store_path):\n",
    "    \"\"\"\n",
    "    加载已保存的文件路径。\n",
    "    参数:\n",
    "        store_path (str): 存储路径\n",
    "    返回:\n",
    "        list: 加载的文件路径列表\n",
    "    \"\"\"\n",
    "    file_paths_file = f'{store_path}/file_paths.pkl'\n",
    "    if os.path.exists(file_paths_file):\n",
    "        return joblib.load(file_paths_file)\n",
    "    return None\n",
    "\n",
    "def file_paths_match(store_path, file_paths):\n",
    "    \"\"\"\n",
    "    检查当前文件路径列表是否与存储路径中的文件路径匹配。\n",
    "    参数:\n",
    "        store_path (str): 存储路径\n",
    "        file_paths (list): 当前文件路径列表\n",
    "    返回:\n",
    "        bool: 是否匹配\n",
    "    \"\"\"\n",
    "    saved_file_paths = load_file_paths(store_path)\n",
    "    return saved_file_paths == file_paths\n",
    "\n",
    "def create_vector_store(docs, store_file, embeddings):\n",
    "    \"\"\"\n",
    "    创建向量存储并保存到本地。\n",
    "    参数:\n",
    "        docs (list): 文档列表\n",
    "        store_file (str): 存储文件路径\n",
    "        embeddings (HuggingFaceEmbeddings): 嵌入模型\n",
    "    返回:\n",
    "        vector_store (FAISS): 向量存储实例\n",
    "    \"\"\"\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    vector_store.save_local(store_file)\n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store(store_path, embeddings):\n",
    "    \"\"\"\n",
    "    从本地加载向量存储。\n",
    "    参数:\n",
    "        store_path (str): 存储路径\n",
    "        embeddings (HuggingFaceEmbeddings): 嵌入模型\n",
    "    返回:\n",
    "        vector_store (FAISS): 向量存储实例\n",
    "    \"\"\"\n",
    "    if os.path.exists(store_path):\n",
    "        vector_store = FAISS.load_local(store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        return vector_store\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_or_create_store(store_path, file_paths, embeddings):\n",
    "    \"\"\"\n",
    "    加载或创建新的向量存储。\n",
    "    参数:\n",
    "        store_path (str): 存储路径\n",
    "        file_paths (list): 文件路径列表\n",
    "        embeddings (HuggingFaceEmbeddings): 嵌入模型\n",
    "    返回:\n",
    "        vector_store (FAISS): 向量存储实例\n",
    "    \"\"\"\n",
    "    if os.path.exists(store_path) and file_paths_match(store_path, file_paths):\n",
    "        print(\"向量数据库与上次使用时一致，无需重新写入\")\n",
    "        vector_store = load_vector_store(store_path, embeddings)\n",
    "        if vector_store:\n",
    "            return vector_store\n",
    "    \n",
    "    print(\"重新写入数据库\")\n",
    "    pages = load_files(file_paths)\n",
    "    docs = split_text(pages)\n",
    "    vector_store = create_vector_store(docs, store_path, embeddings)\n",
    "    \n",
    "    save_file_paths(store_path, file_paths)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def query_vector_store(vector_store: FAISS, query, k=4, relevance_threshold=0.8):\n",
    "    \"\"\"\n",
    "    从向量存储中查询相似文档。\n",
    "    参数:\n",
    "        vector_store (FAISS): 向量存储实例\n",
    "        query (str): 查询内容\n",
    "        k (int): 返回文档数量\n",
    "        relevance_threshold (float): 相关性阈值\n",
    "    返回:\n",
    "        context (list): 查询到的上下文内容\n",
    "    \"\"\"\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": relevance_threshold, \"k\": k})\n",
    "    similar_docs = retriever.invoke(query)\n",
    "    context = [doc.page_content for doc in similar_docs]\n",
    "    return context\n",
    "\n",
    "def load_llm(model_path):\n",
    "    \"\"\"\n",
    "    加载本地的大语言模型。\n",
    "    参数:\n",
    "        model_path (str): 模型路径\n",
    "    返回:\n",
    "        model (AutoModelForCausalLM): 大语言模型实例\n",
    "        tokenizer (AutoTokenizer): 词汇表实例\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "def ask(model, tokenizer: AutoTokenizer, prompt, max_tokens=512):\n",
    "    \"\"\"\n",
    "    生成模型回答。\n",
    "    参数:\n",
    "        model (AutoModelForCausalLM): 大语言模型实例\n",
    "        tokenizer (AutoTokenizer): 词汇表实例\n",
    "        prompt (str): 问题提示\n",
    "        max_tokens (int): 最大生成的token数\n",
    "    返回:\n",
    "        str: 生成的回答\n",
    "    \"\"\"\n",
    "    background_prompt = \"\"\"\n",
    "    You are J.A.R.V.I.S., a highly capable and intelligent private assistant...\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": background_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    terminators = [token for token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")] if token]\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_tokens, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    for token_id in response:\n",
    "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "        print(token, end=\"\", flush=True)\n",
    "        time.sleep(0.05)\n",
    "    print()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def main():\n",
    "    embedding_model = create_embedding_model(Embedding_Model)\n",
    "    vector_store = load_or_create_store(store_path, file_paths, embedding_model)\n",
    "    model, tokenizer = load_llm(LLM_Model)\n",
    "\n",
    "    while True:\n",
    "        qiz = input(\"请输入您的问题：\\n\")\n",
    "        if qiz in ['quit', 'exit']:\n",
    "            print('程序关闭')\n",
    "            break\n",
    "\n",
    "        context = query_vector_store(vector_store, qiz, 4, 0.7)\n",
    "        if not context:\n",
    "            print('找不到匹配的上下文，直接向LLM询问。')\n",
    "            prompt = f'请回答问题：\\n{qiz}\\n'\n",
    "        else:\n",
    "            context = '\\n'.join(context)\n",
    "            prompt = f'根据以下内容：\\n{context}\\n请回答问题：\\n{qiz}\\n'\n",
    "\n",
    "        ans = ask(model, tokenizer, prompt)\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
